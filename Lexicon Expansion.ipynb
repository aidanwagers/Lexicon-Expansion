{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2517e1dc",
   "metadata": {},
   "source": [
    "# Lexicon Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56677e8f",
   "metadata": {},
   "source": [
    "In this notebook, tweets regarding vaping are analyzed against one another on the basis of whether they contain at least one of a set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88b8ae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aidan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "from nltk.book import *\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint # get some prettier printing of objects\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "import sqlite3\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859560da",
   "metadata": {},
   "source": [
    "# Dataset Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bfea96",
   "metadata": {},
   "source": [
    "In this section of the notebook the dataset, a .csv file of vaping related tweets, is read in. Next, the tweets are split into two groups. The first group is tweets containing the word bathroom, and the second is tweets that do not. This process is repeated with an expanding set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70d1d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "with open(\"vaping_tweets.csv\", encoding = \"UTF-8\") as vape_tweets:\n",
    "    tweets = tweets = [tweet.strip() for tweet in vape_tweets.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "de1e68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bathroom_tweets = [tweet for tweet in tweets if \"bathroom\" in tweet]\n",
    "\n",
    "non_bathroom = [tweet for tweet in tweets if \"bathroom\" not in tweet]\n",
    "\n",
    "bathroom_clean = \"\".join(bathroom_tweets)\n",
    "bathroom_clean = [tweet.lower() for tweet in bathroom_clean.split() if tweet.isalpha() and tweet not in sw]\n",
    "\n",
    "bathroom_freq = nltk.FreqDist(bathroom_clean)\n",
    "\n",
    "broomLen = sum([ch for w, ch in bathroom_freq.items()])\n",
    "\n",
    "broom_concentration = dict()\n",
    "\n",
    "for word, count in bathroom_freq.items():\n",
    "    broom_concentration[word] = count/broomLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bff73e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nicotine, kids, vaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2746952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_tweets = [tweet for tweet in tweets if \"bathroom\" in tweet or \"nicotine\" in tweet or \"kids\" in tweet or \"vaping\" in tweet]\n",
    "\n",
    "group1_cleaned = \" \".join(group1_tweets)\n",
    "group1_cleaned = [tweet.lower() for tweet in group1_cleaned.split() if tweet.isalpha() and tweet not in sw]\n",
    "\n",
    "non_group1 = [tweet for tweet in tweets if \"bathroom\" not in tweet and \"nicotine\" not in tweet and \"kids\" not in tweet and \"vaping\" not in tweet]\n",
    "group1_freq = nltk.FreqDist(group1_cleaned)\n",
    "\n",
    "g1Len = sum([ch for w, ch in group1_freq.items()])\n",
    "\n",
    "g1_concentration = dict()\n",
    "\n",
    "for word, count in group1_freq.items():\n",
    "    g1_concentration[word] = count/g1Len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b966e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pod, smoking\n",
    "group2_tweets = [tweet for tweet in tweets if \"bathroom\" in tweet or \"nicotine\" in tweet or \"kids\" in tweet or \"vaping\" in tweet or \"pod\" in tweet or \"smoking\" in tweet]\n",
    "non_group2 = [tweet for tweet in tweets if \"bathroom\" not in tweet and \"nicotine\" not in tweet and \"kids\" not in tweet and \"vaping\" not in tweet and \"pod\" not in tweet and \"smoking\" not in tweet]\n",
    "\n",
    "group2_cleaned = \" \".join(group2_tweets)\n",
    "group2_cleaned = [tweet.lower() for tweet in group2_cleaned.split() if tweet.isalpha() and tweet not in sw]\n",
    "\n",
    "\n",
    "group2_freq = nltk.FreqDist(group2_tweets)\n",
    "\n",
    "g2Len = sum([ch for w, ch in group2_freq.items()])\n",
    "\n",
    "g2_concentration = dict()\n",
    "\n",
    "for word, count in group2_freq.items():\n",
    "    g2_concentration[word] = count/g1Len\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23fa154",
   "metadata": {},
   "source": [
    "Final List: addicted, aspire, stop, bathroom, pod, smoking, nicotine, kids, vaping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0afa7b2",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3805e",
   "metadata": {},
   "source": [
    "Once the final set has been determined, the tweets are once again split into two groups. One containing the words, and the other not. The vapeAnalyzer function is then run against the groups. This produces a dictionary containing summary statistics and comparative statistics of the two groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f59b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordSet_tweets = [tweet for tweet in tweets if \"bathroom\" in tweet or \"nicotine\" in tweet or \"kids\" in tweet or \"vaping\" in tweet or \"pod\" in tweet or \"smoking\" in tweet or \"addicted\" in tweet or \"aspire\" in tweet or \"stop\" in tweet]\n",
    "non_wordSet = [tweet for tweet in tweets if \"bathroom\" not in tweet and \"nicotine\" not in tweet and \"kids\" not in tweet and \"vaping\" not in tweet and \"pod\" not in tweet and \"smoking\" not in tweet and \"aspire\" not in tweet and \"addicted\" not in tweet and \"stop\" not in tweet]\n",
    "\n",
    "\n",
    "set_cleaned = \" \".join(wordSet_tweets)\n",
    "set_cleaned = [tweet.lower() for tweet in set_cleaned.split() if tweet.isalpha() and tweet not in sw]\n",
    "\n",
    "nonSet_cleaned = \" \".join(non_wordSet)\n",
    "nonSet_cleaned = [tweet.lower() for tweet in nonSet_cleaned.split() if tweet.isalpha() and tweet not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "49f39e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vapeAnalyzer(corpus_1,corpus_2,num_words,ratio_cutoff):\n",
    "    combined_dict1 = {}\n",
    "    combined_dict2 = {}\n",
    "    oneV2Dict = {}\n",
    "    twoV1Dict = {}\n",
    "    outputDict = {\"one\":combined_dict1,\"two\":combined_dict2,\"one_vs_two\": oneV2Dict,\"two_vs_one\":twoV1Dict}\n",
    "    \n",
    "    c = Counter()\n",
    "    \n",
    "    total_tokens = 1\n",
    "    unique_tokens = 0\n",
    "    avg_token_len = 0.0\n",
    "    lex_diversity = 0.0\n",
    "    top_10 = Counter()\n",
    "    \n",
    "    #Summary Stats for 1\n",
    "    c1_clean = [word for word in corpus_1]\n",
    "    c1_clean = [word.lower() for word in c1_clean if word.isalpha() and word.lower() not in sw]\n",
    "    \n",
    "    \n",
    "    c.update(c1_clean)\n",
    "        \n",
    "     # Calculate your statistics here\n",
    "    total_tokens = len(c1_clean)\n",
    "    unique_tokens = len(set(c1_clean))\n",
    "    lex_diversity = (unique_tokens/total_tokens)\n",
    "    \n",
    "    \n",
    "    token_len = [len(word) for word in c1_clean]\n",
    "    \n",
    "    avg_token_len = np.mean(token_len)\n",
    "    top_10 = c.most_common(10)\n",
    "    \n",
    "    combined_dict1 = {'tokens':total_tokens,\n",
    "                'unique_tokens':unique_tokens,\n",
    "                'avg_token_length':avg_token_len,\n",
    "                'lexical_diversity':lex_diversity,\n",
    "                'top_words':top_10}\n",
    "    \n",
    "    #Summary Stats for 2\n",
    "    c2_clean = [word for word in corpus_2]\n",
    "    c2_clean = [word.lower() for word in c2_clean if word.isalpha() and word.lower() not in sw]\n",
    "    \n",
    "    \n",
    "    c.update(c2_clean)\n",
    "        \n",
    "     # Calculate statistics here\n",
    "    total_tokens = len(c2_clean)\n",
    "    unique_tokens = len(set(c2_clean))\n",
    "    lex_diversity = (unique_tokens/total_tokens)\n",
    "    \n",
    "    \n",
    "    token_len = [len(word) for word in c2_clean]\n",
    "    \n",
    "    avg_token_len = np.mean(token_len)\n",
    "    top_10 = c.most_common(10)\n",
    "    \n",
    "        # Now we'll fill out the dictionary. \n",
    "    combined_dict2 = {'tokens':total_tokens,\n",
    "                'unique_tokens':unique_tokens,\n",
    "                'avg_token_length':avg_token_len,\n",
    "                'lexical_diversity':lex_diversity,\n",
    "                'top_words':top_10}\n",
    "    \n",
    "    freq1 = nltk.FreqDist(c1_clean)\n",
    "    freq2 = nltk.FreqDist(c2_clean)\n",
    "    \n",
    "    corpus_one_concentration = defaultdict(float)\n",
    "    corpus_two_concentration = defaultdict(float)\n",
    "    \n",
    "    for word, freq in freq1.items():\n",
    "        corpus_one_concentration[word] = freq/len(c1_clean)\n",
    "    \n",
    "    for word, freq in freq2.items():\n",
    "        corpus_two_concentration[word] = freq/len(c2_clean)\n",
    "        \n",
    "    \n",
    "    \n",
    "    oneV2Dict = {}\n",
    "    twoV1Dict = {}\n",
    "    \n",
    "    for word in corpus_one_concentration:\n",
    "        if ratio_cutoff > freq1[word] or ratio_cutoff > freq2[word]:\n",
    "            continue\n",
    "        ratio1 = corpus_one_concentration[word]/corpus_two_concentration[word] \n",
    "        \n",
    "        oneV2Dict[word] = ratio1\n",
    "    \n",
    "    for word in corpus_two_concentration:\n",
    "        if ratio_cutoff > freq1[word] or ratio_cutoff > freq2[word]:\n",
    "            continue\n",
    "        ratio2 = corpus_two_concentration[word]/corpus_one_concentration[word] \n",
    "        \n",
    "        twoV1Dict[word] = ratio2\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    outputDict = {\"Set_Tweets\":combined_dict1,\"Non_Set_Tweets\":combined_dict2,\"Set_vs_Not\": sorted(oneV2Dict.items(), key = lambda item:item[1], reverse = True)[:num_words],\"Not_Vs_Set\":sorted(twoV1Dict.items(), key = lambda item:item[1], reverse = True)[:num_words]}\n",
    "    print(outputDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd78e68",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee15b2",
   "metadata": {},
   "source": [
    "Below is the ouptut of the vapeAnalyzer function. Visible are summary statistics for both sets of tweets, as well as a ratio comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "61522f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Set_Tweets': {'tokens': 2684153, 'unique_tokens': 55266, 'avg_token_length': 5.555872187613747, 'lexical_diversity': 0.02058973538393676, 'top_words': [('juul', 99320), ('smoking', 69123), ('vaping', 56738), ('nicotine', 45649), ('new', 26038), ('vape', 24850), ('like', 24222), ('get', 15779), ('pod', 15454), ('people', 14723)]}, 'Non_Set_Tweets': {'tokens': 1102561, 'unique_tokens': 44369, 'avg_token_length': 5.388470116392653, 'lexical_diversity': 0.04024176440124401, 'top_words': [('juul', 127534), ('smoking', 69123), ('vaping', 56738), ('vape', 55713), ('nicotine', 45649), ('like', 34633), ('new', 31919), ('cigarettes', 26424), ('get', 22653), ('people', 19318)]}, 'Set_vs_Not': [('cleito', 128.77539898060954), ('pockex', 81.60567548372491), ('annie', 79.82568591780473), ('conscience', 48.88125192565402), ('vows', 47.135492928309226), ('bvc', 32.1425038922893), ('carole', 29.71213352343676), ('vgod', 27.99375898095228), ('evo', 27.384454860310374), ('viral', 25.03110327075245)], 'Not_Vs_Set': [('orton', 511.23895185844594), ('mcintyre', 343.26043910495656), ('asuka', 313.07299623331494), ('bayley', 304.9175177155731), ('braun', 252.37351433012174), ('sami', 185.8313015485462), ('owens', 172.84745515214126), ('nia', 158.24062795618565), ('riddle', 152.96594035764613), ('sup', 144.44529116000535)]}\n"
     ]
    }
   ],
   "source": [
    "vapeAnalyzer(set_cleaned,nonSet_cleaned,10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a3808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
